# Experimental Design

## Introduction

Researchers must carefully consider the design of their study before initiating experiments or collecting observational data. After formulating a hypothesis, researchers should answer the following questions: what type of data will need to be collected to assess the hypothesis, what are the spatial and temporal dimensions of the study, what treatments (if any) will be used and how will they be applied, how will study areas or individual animals be selected for study, what sample sizes will be required, what are the potential sources of error which could affect the data collection process, how can these potential sources of error be accounted for, what are the appropriate statistical methods for analyzing the data. The process of experimental design is critical for ensuring that inferences generated from the collected data will properly assess hypotheses. Inappropriate statistical methods are relatively easy to rectify if the study design and data collection are sound; however, there may be little that can be done statistically to rectify a flawed study design. After developing a protocol for the study design and methods of collecting data, solicit review from peers with expert knowledge of your methods or study system. Consulting a statistician when designing a study may also be helpful. Soliciting advice and review from experts can help to improve the design of a study and rectify any overlooked methodological issues before data collection starts. During this process, researchers should carefully consider the three cornerstones of experimental design (Johnson 2002): 1) Randomization, 2) Replication, and 3) Controls.

## Randomization

Randomly assigning treatments to units is a fundamental cornerstone of experimental studies. Indeed, it is the ability of the researcher to randomly assign treatments that separates experimental studies from observational studies. Randomly applying treatments to units is important for preventing conscious or unconscious researcher bias from affecting the results. Imagine a study assessing the hypothesis that providing corn at feeding stations will increase overwinter survival of eastern gray squirrels (Sciurus carolinensis). We have 20 study plots available, 10 of which will be continuously supplemented with dried corn throughout the winter while the other 10 will not. Some study plots are far off the road and require a substantial hike through the woods to reach, while others are situated along the road. Because of the difficulty associated with frequently carrying bags of corn through the woods, we decide to supplement corn at the 10 plots nearest to roads. The problem with this design is that we have introduced bias by selecting experimental plots nearest to the roads instead of at random. Squirrels living near the road may naturally suffer higher rate of mortality from car collisions irrespective of food abundance, so our data may show lower survival in the supplemented plots. We would erroneously conclude that corn supplementation decreases survival of squirrels, when in fact our results are a consequence of poor study design. A more appropriate design would be to randomly select plots at which to supplement corn regardless of difficulty of access. Treatments can be randomly selected using a random number table or random number generator, which are widely available online and included in many software packages.

True randomization is not possible for observational studies, as treatments cannot be applied to units. Surveying abundance of western capercaillie (Tetrao urogallus) across an elevational gradient would be an observational study because researchers can not select study locations and randomly assign some to be at a high elevation and some to be at a low elevation. This is not to say that randomization is unimportant for observational studies, as other aspects of randomization besides treatment assignment should be incorporated into the design of both observational and experimental studies to reduce bias. We may not be able to randomly assign the elevation of potential study locations in the grouse example; however, if we have a pool of potential survey areas at high and low elevations, we could randomly select a subset of study locations within each elevation band to survey. 

## Replication

Replication is the process of conducting experimental manipulations or observations on multiple, independent units. Appropriate levels of replication must be enacted within all combinations of treatments groups or other explanatory variables relevant to the research hypotheses. For example, if a researcher is interested in studying the interactions between food supplementation and predator removal on population growth rates of northern bobwhite (Colinus virginianus) they would need to establish multiple study plots for each of the four combinations of food and predator control treatments (food supplemented/predators removed, no food supplemented/predators removed, food supplemented/no predators removed, no food supplemented/no predators removed). Replication is an essential component of both experimental and observational studies for several reasons. Collecting data from replicate units within the same treatment group allows researchers to assess the variance among units in the group, which is a prerequisite for all statistical analyses. Moreover, replication reduces the effects of random chance on the measured data. Researchers are generally not able to measure every individual in a population or every potential study area across the landscape, and instead rely on sampling a subset of the available to estimate the population mean. Environmental systems are complex and variable over space, time, and between individuals, so single observations may represent unusual outcomes rather than the population mean. Imagine that we are interested in estimating the juvenile survival and sources of fawn mortality for a mule deer population (Odocoileus hemionus) and we deploy a radiocollar on a single fawn. If we found that this fawn was killed by a predator, we would not conclude that all fawns in this population are killed by predators. If instead we collared 100 fawns and found that 55 survived, 31 were killed by predators, 6 were killed by vehicle collisions, and 8 died from starvation or disease, the higher number of replicates would allow us to be much more confident that our results are representative of the actual population values.

Increasing the number of replicates will increase the precision of population estimates; however, increasing the sample size comes with increased costs and effort. An important question when considering experimental design is then: what is the sufficient level of replication that balances precision of estimates with the cost and effort of conducting the study? The minimum sample size will vary by study, though it may be estimated by performing a prospective power analysis during the design phase. The minimum required sample size can be estimated with a prospective power analysis based on two parameters: 1) the magnitude of the difference between treatment groups that is desired or is expected to be observed in the study, and 2) the variance between units within treatment groups that is expected to be observed in the study (see Steidl & Thomas (2001) for a detailed description of power analyses). The question, then, is how to estimate the variance and difference between treatment groups in your study before collecting any data. Researchers can use estimates derived from published literature using a similar study system with the same or similar species. For example, researchers designing a study on the effects of predator removal on the survival of northern bobwhites in Iowa may be able to estimate the minimum required radiotransmitters to deploy based on published estimates of the variance and survival of bobwhites in high-predator and low-predator habitats from Georgia.  If time and funds are sufficient, researchers could also perform a small-scale test of the methods and data-collection procedures before the main study. These pilot studies are valuable power analyses by collecting preliminary data on the group means and variance that are likely to be observed in the main study. In addition to informing prospective power analyses, pilot studies provided the added benefit of allowing researchers to assess the feasibility of the study design and identify previously-overlooked methodological issues before the main study commences.
  	
**Comment: I tried to keep the discussion of the power analyses simple here so I omitted discussing type I and II error rates (since those are set by the researcher in the design phase). I could bring them up, but would require a much more in-depth discussion. Are error rates brought up anywhere else? Also, I could add a section discussing how retrospective power analyses are not really valid, though this would require discussion of power.**
  	
Though replication is imperative in scientific studies, researchers must be careful to avoid pseudoreplication. True experimental replicates are independent of each other; however, pseudoreplication occurs when measurements which are not independent are treated as independent in statistical analyses (Hurlbert 1984). The consequences of pseudoreplication include overestimating precision and falsely concluding that there is a treatment effect when there is none (Heffner et al. 1996). Preventing pseudoreplication requires identifying the experimental unit of a study, which is the lowest experimental level in which units are independent from others. The experimental unit is the level at which the treatment is applied in experimental studies, though care must be taken when identifying the experimental unit in studies with multiple levels of treatments. Imagine a study where different herbicides are sprayed on a series of forest patches, and within each  patch 4 study plots are established and each one is selectively logged at a different thinning rate. The logged plots within each forest are not independent as they are exposed to the same broad herbicide application. The individual forest patches are the experimental unit of analysis while the logged plots are considered subunits. Importantly, increasing the number of subunits will not increase the levels of replication and statistical power of a study; replication is increased only by increasing the number of experimental units.
  	 	
In some cases, non-independence between observations may be readily apparent. The locations of individual bison (Bison bison) in the same herd would clearly not be independent of one another, and treating them as independent would constitute pseudoreplication. In other cases, pseudoreplication may be less obvious. Imagine that we are designing a study to examine the effects of selective logging on the body mass of ruffed grouse (Bonasa umbellus). We identify a forest stand which has been recently been logged and a forest stand which has not been logged, and in each stand we capture and weigh 30 grouse. We find that the average mass of the 30 grouse in the logged stand is 377 g and the average mass of the 30 grouse in the logged stand is 363 g. Doing a t-test with a sample size of 30 for each treatment, we get a p-value of 0.03 and conclude that grouse are heavier in selectively-logged stands. The issue is that, even though we measured 60 grouse, the treatment (presence or absence of logging) is applied at the level of the forest stand. Grouse within a stand are exposed to the same logging application and so are not independent. The individual forest stands are the experimental units of analysis while individual grouse are subunits. Even though we have measured 60 grouse, we only have one forest stand in each treatment. We have no replication at the level of the experimental unit, and applying a statistical analysis is inappropriate. By treating each of the 60 grouse as independent, we have vastly overestimated our sample size. Capturing and weighing more grouse in each patch will not help to resolve this issue; the only way to increase the number of replicates is expand our study area to weigh grouse in more logged and unlogged stands. An appropriate method to avoid pseudoreplication would be to identify 10 stands which have been recently logged at similar thinning rates and 10 stands which have not been logged. In each one, we capture and weigh 30 grouse. We then average the weights across grouse in each stand to get a single weight value for each stand. We can then perform a t-test between the average weights of grouse in the 10 logged stands and the 10 unlogged stands.
  	 	
**Comment: I could bring up the nested design, but functionally this is the same thing. Also, should I assume that the audience will know about t-tests and p-values?**
  	 	
Though subunits should not be treated as independent replicates, increasing the number of subunit measurements provides other benefits for statistical analyses. Sometimes, the variance within experimental units is of direct interest for studies, e.g., assessing how the variation in grouse weight rather than mean weight differs between logged and unlogged stands. In this case, measuring multiple subunits within each experimental unit would be required to estimate variance. Taking repeat measurements within experimental units also helps to reduce stochastic error and more accurately estimate the mean response in the experimental unit. If we had only captured and weighed one grouse in each forest stand, the single grouse may be very light or very heavy and not represent the population average. Measuring 30 birds at each site, however, would likely lead to more accurate estimates of the stand-level average. The important distinction to remember is that increasing the number of subunits may increase the accuracy of measurements within an experimental unit but does not increase the number of replicates or statistical power.
  	 	
**Comment: Should I add a paragraph about issues with generalizing studies without sufficient spatial/temporal replication?**

## Controls

**Still working on this section**